{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97326a7",
   "metadata": {},
   "source": [
    "# Current Topics and Projects\n",
    "## 1. Data retrieving (Reichelt, Tagesschau, Jung&Naiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1db005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the first file to run which requests the data, be aware of the limits of the key.\n",
    "\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "#from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import spacy\n",
    "from itertools import product\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c7d9221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# private api_key\n",
    "# Peer: \n",
    "api_key_peer1 = '' \n",
    "api_key_peer2 = '' \n",
    "api_key_peer3 = ''\n",
    "\n",
    "# Raphaela:\n",
    "api_key = '' \n",
    "\n",
    "# Define the base API URL\n",
    "base_url = \"https://www.googleapis.com/youtube/v3/channels\"\n",
    "\n",
    "# Set the API endpoint\n",
    "search_endpoint = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "\n",
    "# Set the API endpoint\n",
    "video_endpoint = 'https://www.googleapis.com/youtube/v3/videos'\n",
    "\n",
    "# Set the API endpoint\n",
    "caption_endpoint = \"https://www.googleapis.com/youtube/v3/captions\"\n",
    "\n",
    "# Set the API endpoint\n",
    "comment_endpoint = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
    "\n",
    "# Set the API endpoint\n",
    "replies_endpoint = \"https://www.googleapis.com/youtube/v3/comments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7645c38f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set date, in order to get not the newest videos without comments\n",
    "# now = datetime.now(timezone.utc)\n",
    "# get timestamp of 5 days earlier\n",
    "# five_days_ago = now - timedelta(days=10)\n",
    "# change format\n",
    "# datetime_string = five_days_ago.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "# print(datetime_string)\n",
    "\n",
    "# take 31. of May as stable limit\n",
    "datetime_string_end = '2023-01-1T15:07:51Z'\n",
    "datetime_string_sart = '2022-01-1T15:07:51Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d4f33de-877f-49ce-98c5-01aa8672f973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the parameters for the API request\n",
    "def video_request(key, channel_id, endpoint):\n",
    "    '''\n",
    "    Function to get information about videos\n",
    "    '''\n",
    "    params = {\n",
    "        \"key\": key,\n",
    "        \"channelId\": channel_id,\n",
    "        \"part\": \"snippet\",\n",
    "        \"order\": \"date\",\n",
    "        \"type\": \"video\",\n",
    "        \"maxResults\": 50,\n",
    "        \"publishedBefore\": datetime_string_end,\n",
    "        \"publishedAfter\" : datetime_string_sart\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Send the API request\n",
    "    response = requests.get(endpoint, params=params)\n",
    "\n",
    "    # Parse the response and store video information in a dictionary\n",
    "    video_dict = {}\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        videos = data[\"items\"]\n",
    "\n",
    "        while \"nextPageToken\" in data:\n",
    "            next_page_token = data[\"nextPageToken\"]\n",
    "            params[\"pageToken\"] = next_page_token\n",
    "            response = requests.get(endpoint, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                videos += data[\"items\"]\n",
    "                \n",
    "            else:\n",
    "                print(\"Error occurred while fetching comments.\")\n",
    "                print(response.status_code)\n",
    "                break\n",
    "                    \n",
    "\n",
    "        # Process the video information\n",
    "        for video in videos:\n",
    "            video_id = video[\"id\"][\"videoId\"]\n",
    "            video_title = video[\"snippet\"][\"title\"]\n",
    "            video_published_at = video[\"snippet\"][\"publishedAt\"]\n",
    "            \n",
    "            # Store the video information in the dictionary\n",
    "            video_dict[video_id] = {\n",
    "                \"title\": video_title,\n",
    "                \"published_at\": video_published_at\n",
    "            }\n",
    "\n",
    "    else:\n",
    "        print(\"Error occurred while fetching the videos.\")\n",
    "        print(response.status_code)\n",
    "\n",
    "    video_ids = []\n",
    "\n",
    "    for video_id, video_info in video_dict.items():\n",
    "        video_ids.append(video_id)\n",
    "\n",
    "    return video_dict, video_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38b14113-89a6-407c-886f-a4228639587a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def video_transscrip(vid): \n",
    "    '''\n",
    "    This function utilizes the YouTubeTranscriptApi as the Youtube Data Api from google doesnt provide transcripts\n",
    "    '''\n",
    "    try:\n",
    "        # Retrieve the transcript\n",
    "        transcript = YouTubeTranscriptApi.get_transcripts([vid], languages=['de'])\n",
    "\n",
    "        # Combine transcript text into a single string\n",
    "        transcript_text = ' '.join(entry['text'] for entry in transcript[0][vid])\n",
    "\n",
    "    except: \n",
    "        print(f'Video {vid} has no transcript')\n",
    "        return 0 \n",
    "      \n",
    "            \n",
    "    return transcript_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c98fe5a-803d-48f6-9e99-cca09badab66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def video_stats(api_key, video_ids, endpoint):\n",
    "    '''\n",
    "     Function to retrieve the video statistics for certain video_ids via the API\n",
    "    '''\n",
    "    # Initialize an empty dictionary to store the results\n",
    "    vid_stats = {}\n",
    "\n",
    "    params = {\n",
    "        'part': 'snippet,statistics,contentDetails',\n",
    "        'key': api_key\n",
    "    }\n",
    "\n",
    "    # Process each batch of video IDs\n",
    "    for i in video_ids:\n",
    "        params['id'] = i\n",
    "        response = requests.get(endpoint, params=params)\n",
    "        if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                vid_stats[i] = data\n",
    "                \n",
    "        else:\n",
    "            print(\"Error occurred while fetching videos.\")\n",
    "            print(response.status_code)\n",
    "            \n",
    "    return vid_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7cf117b3-17b8-4e97-8a35-0f2b45eccd49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def comment_request(api_key, video_ids, endpoint, replies_endpoint):\n",
    "    '''\n",
    "    Function to retrieve the comments for certain video_ids via the API\n",
    "    '''\n",
    "\n",
    "    # Dictionary to store comment data\n",
    "    comments_dict = {}\n",
    "\n",
    "    # Iterate over the video IDs\n",
    "    for video_id in video_ids:\n",
    "        # Set the parameters for the API request\n",
    "        params = {\n",
    "            \"key\": api_key,\n",
    "            \"videoId\": video_id,\n",
    "            \"part\": \"id,snippet,replies\",\n",
    "            \"maxResults\": 100  # Set the desired number of comments per video\n",
    "        }\n",
    "\n",
    "        # Send the API request\n",
    "        response = requests.get(endpoint, params=params)\n",
    "        \n",
    "        # Parse the response and retrieve comment information\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            comment_thread = data[\"items\"]\n",
    "            while \"nextPageToken\" in data:\n",
    "                next_page_token = data[\"nextPageToken\"]\n",
    "                params[\"pageToken\"] = next_page_token\n",
    "                response = requests.get(endpoint, params=params)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    comment_thread += data[\"items\"]\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Error occurred while fetching comments.\")\n",
    "                    print(response.status_code)\n",
    "                    break\n",
    "        else:\n",
    "            print(\"Error occurred while fetching comments for video ID.\")\n",
    "            print(response.status_code)\n",
    "        \n",
    "        for i in comment_thread:\n",
    "            if i['snippet']['totalReplyCount'] > 0:\n",
    "                if i['snippet']['totalReplyCount'] > len(i['replies'][\"comments\"]):           \n",
    "                    replies_params = {\n",
    "                        \"key\": api_key,\n",
    "                        \"part\": \"snippet\",\n",
    "                        \"parentId\": i[\"id\"]}\n",
    "                    replies_response = requests.get(replies_endpoint, params=replies_params)\n",
    "                    data = replies_response.json()\n",
    "                    i[\"replies\"][\"comments\"] = data\n",
    "            \n",
    "        comments_dict[video_id] =  comment_thread\n",
    "\n",
    "    return comments_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "659d8644-523e-4d7c-a93e-1840fee6b1fd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 19/228 [00:09<01:45,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video YOVcvB0Qyk4 has no transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 187/228 [01:40<00:20,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video P3TNooHlCF4 has no transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 219/228 [01:57<00:04,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video 5mLwMCrjZR8 has no transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 228/228 [02:02<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 58/495 [00:34<04:04,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video -RjfBT2tEIk has no transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 130/495 [01:20<03:32,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video 2N5bNh-uKHs has no transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 142/495 [01:27<03:12,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video _2yMkTe5ypM has no transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 177/495 [01:48<03:06,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video GLUdfBl0oGo has no transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 264/495 [02:42<02:11,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video p09tj1QqCFs has no transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 315/495 [03:13<01:47,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video wN_XFvqu8uI has no transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 383/495 [03:58<01:02,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video 68bQqiJte5U has no transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 450/495 [04:42<00:28,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video PqGyFtwNXt0 has no transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 483/495 [05:02<00:06,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video HB0UWu5isng has no transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 485/495 [05:03<00:05,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video PrIkPu3wAk8 has no transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [05:10<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/278 [00:00<02:06,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video hNp_XJUZEfU has no transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 106/278 [01:11<01:42,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video mL2TWzrvlYE has no transcript\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 278/278 [03:02<00:00,  1.52it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The following code requests all the videos and transcripts of videos in 2022. Additionaly we calculate the cosine simularity \n",
    "of each combinations and pick the 50 best unique combinations \n",
    "'''\n",
    "\n",
    "channel_ids = [\"UCcoQ3WG2J_Xjwwyt-sJqh-w\", 'UCMIgOXM2JEQ2Pv2d0_PVfcg', 'UCv1WDP5EiipMQ__C4Cg6aow']\n",
    "\n",
    "merged_trans = []\n",
    "merged_id = []\n",
    "merged_channel = []\n",
    "\n",
    "#get all videos 2022\n",
    "for i in channel_ids:\n",
    "    video_dict, video_ids = video_request(api_key_peer3, i, search_endpoint)\n",
    "    print(len(video_ids))\n",
    "    progress_bar = tqdm(video_ids, total=len(video_ids))\n",
    "    for x in progress_bar:\n",
    "        trans = video_transscrip(x)\n",
    "        merged_trans.append(trans)\n",
    "        merged_id.append(x)\n",
    "        merged_channel.append(i)\n",
    "        \n",
    "# Create a DataFrame from the lists\n",
    "data = {'video_id': merged_id, 'channel_id': merged_channel, 'transcript': merged_trans}\n",
    "\n",
    "#drop na and cache data\n",
    "df = pd.DataFrame(data)\n",
    "df = df.dropna(how='all')\n",
    "df = df[df['transcript'] != '0']\n",
    "\n",
    "df.to_csv(\"data/raw/vids.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d22c2254-6954-4965-904a-d007f7e0ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112860/112860 [4:47:06<00:00,  6.55it/s]  \n",
      "100%|██████████| 63384/63384 [12:05:38<00:00,  1.46it/s]  \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This piece of code calculates the cosine simuarity odf all videos from channel A to B and from channel A to C \n",
    "and then picks the closes combination of ABC videos by merging via A. This is computational expensiv but still better then check for every\n",
    "ABC combination\n",
    "'''\n",
    "# Load the SpaCy model (make sure to download and install the model beforehand)\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "# Get unique channelIds\n",
    "unique_channelIds = df[\"channel_id\"].unique()\n",
    "\n",
    "# Dictionary to store the best matches for each channel\n",
    "matches1 = {}\n",
    "matches2 = {}\n",
    "\n",
    "# Get video IDs and transcripts for each channelId in the combination\n",
    "video_data = [df[df['channel_id'] == channelId]['video_id'].tolist() for channelId in unique_channelIds]\n",
    "\n",
    "# Generate combinations of video data within each channelId\n",
    "video_combinationsAB = list(product(video_data[0], video_data[1]))\n",
    "video_combinationsAC = list(product(video_data[0], video_data[2]))\n",
    "\n",
    "# Define a function to calculate similarity\n",
    "def calculate_similarity(transcripts):\n",
    "    return nlp(transcripts[0]).similarity(nlp(transcripts[1]))\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures1 = {combination: executor.submit(calculate_similarity, [df[df['video_id'] == video][\"transcript\"].values[0] for video in combination]) for combination in video_combinationsAB}\n",
    "    futures2 = {combination: executor.submit(calculate_similarity, [df[df['video_id'] == video][\"transcript\"].values[0] for video in combination]) for combination in video_combinationsAC}\n",
    "    \n",
    "    for combination, future in tqdm(futures1.items()):\n",
    "        matches1[combination] = future.result()\n",
    "\n",
    "    for combination, future in tqdm(futures2.items()):\n",
    "        matches2[combination] = future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0564498-7b17-40c8-8310-6ea962200fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lists saved to files: Data/ids/vid_id_jung.txt Data/ids/vid_id_dw.txt Data/ids/vid_id_reichelt.txt\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This piece of code sorts the matches and picks the best and unique video ids\n",
    "'''\n",
    "\n",
    "# Step 1: Sort dictionaries based on similarity\n",
    "sorted_matches1 = {k: v for k, v in sorted(matches1.items(), key=lambda item: item[1], reverse=True)}\n",
    "sorted_matches2 = {k: v for k, v in sorted(matches2.items(), key=lambda item: item[1], reverse=True)}\n",
    "sorted_matches2 = {key: value for key, value in sorted_matches2.items() if value != 1.0}\n",
    "sorted_matches1 = {key: value for key, value in sorted_matches1.items() if value != 1.0}\n",
    "\n",
    "# Convert tuples to strings for keys\n",
    "sort1_str = {str(key): value for key, value in sorted_matches1.items()}\n",
    "sort2_str = {str(key): value for key, value in sorted_matches2.items()}\n",
    "\n",
    "# Save the filtered dictionary to a JSON file\n",
    "with open(\"data/raw/sort2\", 'w') as outfile:\n",
    "    json.dump(sort2_str, outfile, indent=4)\n",
    "    \n",
    "# Save the filtered dictionary to a JSON file\n",
    "with open(\"data/raw/sort1\", 'w') as outfile:\n",
    "    json.dump(sort1_str, outfile, indent=4)\n",
    "\n",
    "def filter_best_matches(matches):\n",
    "    best_matches = {}  # Initialize an empty dictionary to store the best matches\n",
    "    \n",
    "    for (a, b), value in matches.items():\n",
    "        # Check if both a and b are completely new\n",
    "        if a not in best_matches.keys() and b not in [v[0] for v in best_matches.values()]:\n",
    "            # Add the new entry to best_matches\n",
    "            best_matches[a] = (b, value)\n",
    "    \n",
    "    return best_matches\n",
    "\n",
    "\n",
    "filtered_matches1 = filter_best_matches(sorted_matches1)\n",
    "filtered_matches2 = filter_best_matches(sorted_matches2)\n",
    "\n",
    "\n",
    "vid_id_jung = []\n",
    "vid_id_dw = []\n",
    "vid_id_reichelt = []\n",
    "\n",
    "for key1, (b1, value1) in filtered_matches1.items():\n",
    "    for key2, (b2, value2) in filtered_matches2.items():\n",
    "        if (\n",
    "            key1 == key2\n",
    "            and len(vid_id_reichelt) <= 50\n",
    "            and key1 not in vid_id_reichelt\n",
    "            and b1 not in vid_id_dw\n",
    "            and b2 not in vid_id_jung\n",
    "        ):\n",
    "            vid_id_reichelt.append(key1)\n",
    "            vid_id_dw.append(b1)\n",
    "            vid_id_jung.append(b2)\n",
    "            \n",
    "# Specify the file paths where you want to save the lists\n",
    "output_file_jung = 'data/ids/vid_id_jung.txt'\n",
    "output_file_dw = 'data/ids/vid_id_dw.txt'\n",
    "output_file_reichelt = 'data/ids/vid_id_reichelt.txt'\n",
    "\n",
    "# Write the contents of the lists to the respective files\n",
    "with open(output_file_jung, 'w') as f:\n",
    "    for item in vid_id_jung:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(output_file_dw, 'w') as f:\n",
    "    for item in vid_id_dw:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(output_file_reichelt, 'w') as f:\n",
    "    for item in vid_id_reichelt:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "print(\"Lists saved to files:\", output_file_jung, output_file_dw, output_file_reichelt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4d012cb-edc4-412e-aeb9-7b914d2847e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 done\n",
      "Step2 done\n",
      "Step3 done\n",
      "Step4 done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. Julian Reichelt\n",
    "'''\n",
    "\n",
    "# Set the channel ID of AchtungReichelt\n",
    "channel_name = \"Achtung, Reichelt!\"\n",
    "\n",
    "channel_id = \"UCcoQ3WG2J_Xjwwyt-sJqh-w\"\n",
    "\n",
    "# Prepare the parameters for the API request\n",
    "params = {\n",
    "    \"part\": \"statistics\",\n",
    "    \"id\": channel_id,\n",
    "    \"part\": \"snippet\",\n",
    "    \"key\": api_key\n",
    "}\n",
    "\n",
    "response = requests.get(base_url, params=params)\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Extract the statistics for each channel\n",
    "    data = response.json()\n",
    "    print(\"Step1 done\")\n",
    "    \n",
    "\n",
    "\n",
    "# Initialize a list to store the lines from the text file\n",
    "vid_id_reichelt = []\n",
    "\n",
    "# Open the text file and read its contents\n",
    "with open(\"data/ids/vid_id_reichelt.txt\", 'r', encoding='utf-8') as txtfile:\n",
    "    vid_id_reichelt  = [line.strip() for line in txtfile.readlines()]    \n",
    "\n",
    "print(\"Step2 done\")\n",
    "comments_dict_reichelt = comment_request(api_key_peer1, vid_id_reichelt, comment_endpoint, replies_endpoint)\n",
    "print(\"Step3 done\")\n",
    "video_statistics = video_stats(api_key_peer1, vid_id_reichelt, video_endpoint)\n",
    "print(\"Step4 done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37431f63-f503-4dea-aaf0-eb983a524e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the comments_dict to a file\n",
    "filename = \"data/raw/comments_dict_reichelt.json\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    json.dump(comments_dict_reichelt, file)\n",
    "\n",
    "\n",
    "# Save vid_stats as JSON file\n",
    "with open('data/raw/video_stat_reichelt.json', 'w') as json_file:\n",
    "    json.dump(video_statistics, json_file)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8742467b-ce7c-4f63-853d-99448e1d9f55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 done\n",
      "Step2 done\n",
      "Step3 done\n",
      "Step4 done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2. dw\n",
    "'''\n",
    "channel_id = 'UCMIgOXM2JEQ2Pv2d0_PVfcg'\n",
    "\n",
    "# Prepare the parameters for the API request\n",
    "params = {\n",
    "    \"part\": \"statistics\",\n",
    "    \"id\": channel_id,\n",
    "    \"part\": \"snippet\",\n",
    "    \"key\": api_key\n",
    "}\n",
    "\n",
    "response = requests.get(base_url, params=params)\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Extract the statistics for each channel\n",
    "    data = response.json()\n",
    "    print(\"Step1 done\")\n",
    "    \n",
    "    \n",
    "# Initialize a list to store the lines from the text file\n",
    "vid_id_dw = []\n",
    "\n",
    "# Open the text file and read its contents\n",
    "with open(\"data/ids/vid_id_dw.txt\", 'r', encoding='utf-8') as txtfile:\n",
    "    vid_id_dw  = [line.strip() for line in txtfile.readlines()]\n",
    "\n",
    "    \n",
    "print(\"Step2 done\")\n",
    "comments_dict_dw = comment_request(api_key, vid_id_dw, comment_endpoint,replies_endpoint)\n",
    "print(\"Step3 done\")\n",
    "video_statistics_dw = video_stats(api_key, vid_id_dw, video_endpoint)\n",
    "print(\"Step4 done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6560f2ed-0dcc-400e-97ba-55f4526865d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving done\n"
     ]
    }
   ],
   "source": [
    "# Save the comments_dict to a file\n",
    "filename = \"data/raw/comments_dict_dw.json\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    json.dump(comments_dict_dw, file)\n",
    "\n",
    "\n",
    "# Save vid_stats as JSON file\n",
    "with open('data/raw/video_stat_dw.json', 'w') as json_file:\n",
    "    json.dump(video_statistics_dw, json_file)\n",
    "    \n",
    "print(\"saving done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e02783f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 done\n",
      "Step2 done\n",
      "Step3 done\n",
      "Step4 done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "3. Jung&Naiv\n",
    "'''\n",
    "channel_id = 'UCv1WDP5EiipMQ__C4Cg6aow'\n",
    "\n",
    "# Prepare the parameters for the API request\n",
    "params = {\n",
    "    \"part\": \"statistics\",\n",
    "    \"id\": channel_id,\n",
    "    \"part\": \"snippet\",\n",
    "    \"key\": api_key\n",
    "}\n",
    "\n",
    "response = requests.get(base_url, params=params)\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Extract the statistics for each channel\n",
    "    data = response.json()\n",
    "    print(\"Step1 done\")\n",
    "\n",
    "    \n",
    "# Initialize a list to store the lines from the text file\n",
    "vid_id_jung = []\n",
    "\n",
    "# Open the text file and read its contents\n",
    "with open(\"data/ids/vid_id_jung.txt\", 'r', encoding='utf-8') as txtfile:\n",
    "    vid_id_jung  = [line.strip() for line in txtfile.readlines()]\n",
    "    \n",
    "print(\"Step2 done\")\n",
    "comments_dict_jung = comment_request(api_key_peer3,vid_id_jung, comment_endpoint,replies_endpoint)\n",
    "print(\"Step3 done\")\n",
    "video_statistics_jung = video_stats(api_key_peer3, vid_id_jung, video_endpoint)\n",
    "print(\"Step4 done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8e443e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data saved\n"
     ]
    }
   ],
   "source": [
    "# Save the comments_dict to a file\n",
    "filename = \"data/raw/comments_dict_jung.json\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    json.dump(comments_dict_jung, file)\n",
    "\n",
    "\n",
    "\n",
    "# Save vid_stats as JSON file\n",
    "with open('Data/raw/video_stat_jung.json', 'w') as json_file:\n",
    "    json.dump(video_statistics_jung, json_file)\n",
    "    \n",
    "print(\"data saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
